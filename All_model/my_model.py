from tensorflow.keras.layers import Dense, Input, Flatten, Dropout, LSTM,\
    BatchNormalization,Bidirectional,TimeDistributed,SpatialDropout1D,\
    GlobalAveragePooling1D,ZeroPadding1D,Conv1D, MaxPooling1D, Embedding,Conv2D,MaxPooling2D,Concatenate,Multiply,add,LeakyReLU
from .transformer import TransformerBlock,TokenAndPositionEmbedding
from tensorflow.keras import layers
from .selfattention import *

def mine(embedding_layer,personality_embedding_layer):
    # milti_head_attention1 = mul_att(embedding_layer,embedding_layer)
    milti_head_attention1 = Self_Attention(300)(embedding_layer)
    Bi_LSTM_Layer1 = Bidirectional(LSTM(200,return_sequences=False, dropout=0.2,
                                        recurrent_dropout=0.2,activation = 'tanh'))(milti_head_attention1)
    Conv_layer1 = Conv1D(128, 1, padding='valid', activation='relu', strides=1)(milti_head_attention1)
    Conv_layer2 = Conv1D(128, 4, padding='valid', activation='relu', strides=1)(milti_head_attention1)
    Conv_layer3 = Conv1D(128, 9, padding='valid', activation='relu', strides=1)(milti_head_attention1)
    Maxpooling_layer1 = GlobalAveragePooling1D()(Conv_layer1)
    Maxpooling_layer2 = GlobalAveragePooling1D()(Conv_layer2)
    Maxpooling_layer3 = GlobalAveragePooling1D()(Conv_layer3)
    Concatenate_layer1 = Concatenate()([Maxpooling_layer1,Maxpooling_layer2])
    Concatenate_layer2 = Concatenate()([Concatenate_layer1,Maxpooling_layer3])
    Concatenate_layer2 = Concatenate()([Bi_LSTM_Layer1,Concatenate_layer2])
    Concatenate_layer2 = Dropout(0.2)(Concatenate_layer2)

    # milti_head_attention2 = mul_att(personality_embedding_layer,personality_embedding_layer)
    milti_head_attention2 = Self_Attention(300)(personality_embedding_layer)
    Bi_LSTM_Layer2 = Bidirectional(LSTM(200,return_sequences=False, dropout=0.2,
                                        recurrent_dropout=0.2,activation = 'tanh'))(milti_head_attention2)
    Conv_layer4 = Conv1D(128, 1, padding='valid', activation='relu', strides=1)(milti_head_attention2)
    Conv_layer5 = Conv1D(128, 4, padding='valid', activation='relu', strides=1)(milti_head_attention2)
    Conv_layer6 = Conv1D(128, 9, padding='valid', activation='relu', strides=1)(milti_head_attention2)
    Maxpooling_layer4 = GlobalAveragePooling1D()(Conv_layer4)
    Maxpooling_layer5 = GlobalAveragePooling1D()(Conv_layer5)
    Maxpooling_layer6 = GlobalAveragePooling1D()(Conv_layer6)
    Concatenate_layer3 = Concatenate()([Maxpooling_layer4,Maxpooling_layer5])
    Concatenate_layer4 = Concatenate()([Concatenate_layer3,Maxpooling_layer6])
    Concatenate_layer4 = Concatenate()([Concatenate_layer4,Bi_LSTM_Layer2])
    Concatenate_layer4 = Dropout(0.2)(Concatenate_layer4)

    Concatenate_layer = Concatenate()([Concatenate_layer2,Concatenate_layer4])
    Flatten_layer = Flatten()(Concatenate_layer)
    # transformer_block = TransformerBlock(embed_dim=embedding_size,num_heads=2,ff_dim=32)
    # Transformer_layer1 = transformer_block(position_embedding_layer)
    # Global_AVG_Pooling_layer1 = GlobalAveragePooling1D()(Transformer_layer1)
    # Drop_layer1 = Dropout(0.5)(Global_AVG_Pooling_layer1)
    Drop_layer1 = Dropout(0.3)(Flatten_layer)
    # Dense_layer1 = Dense(64, activation='relu')(Drop_layer1)
    # Dense_layer2 = Dense(32, activation='relu')(Dense_layer1)
    Dense_layer1 = Dense(64, activation='tanh')(Drop_layer1)
    Dense_layer2 = Dense(32, activation='tanh')(Dense_layer1)
    # Dense_layer1 = Dense(64)(Drop_layer1)
    # Dense_layer1 = LeakyReLU()(Dense_layer1)
    # Dense_layer2 = Dense(32)(Dense_layer1)
    # Dense_layer2 = LeakyReLU()(Dense_layer2)
    output1 = Dense(1, activation='sigmoid')(Dense_layer2)
    output2 = Dense(1, activation='sigmoid')(Dense_layer2)
    output3 = Dense(1, activation='sigmoid')(Dense_layer2)
    output4 = Dense(1, activation='sigmoid')(Dense_layer2)
    output5 = Dense(1, activation='sigmoid')(Dense_layer2)
    return output1,output2,output3,output4,output5



def mine2(embedding_layer,personality_embedding_layer):
    # milti_head_attention1 = mul_att(embedding_layer,embedding_layer)
    milti_head_attention1 = Self_Attention(300)(embedding_layer)
    Bi_LSTM_Layer1 = Bidirectional(LSTM(200,return_sequences=True, dropout=0.2,
                                        recurrent_dropout=0.2,activation = 'tanh'))(milti_head_attention1)
    Conv_layer99 = Conv1D(128, 1, padding='valid', activation='relu', strides=1)(Bi_LSTM_Layer1)
    Conv_layer98 = Conv1D(128, 4, padding='valid', activation='relu', strides=1)(Bi_LSTM_Layer1)
    Conv_layer97 = Conv1D(128, 9, padding='valid', activation='relu', strides=1)(Bi_LSTM_Layer1)
    Maxpooling_layer99 = GlobalAveragePooling1D()(Conv_layer99)
    Maxpooling_layer98 = GlobalAveragePooling1D()(Conv_layer98)
    Maxpooling_layer97 = GlobalAveragePooling1D()(Conv_layer97)
    Concatenate_layer99 = Concatenate()([Maxpooling_layer99,Maxpooling_layer98])
    Concatenate_layer98 = Concatenate()([Concatenate_layer99,Maxpooling_layer97])
    Concatenate_layer98 = Dropout(0.2)(Concatenate_layer98)

    Conv_layer1 = Conv1D(128, 1, padding='valid', activation='relu', strides=1)(milti_head_attention1)
    Conv_layer2 = Conv1D(128, 4, padding='valid', activation='relu', strides=1)(milti_head_attention1)
    Conv_layer3 = Conv1D(128, 9, padding='valid', activation='relu', strides=1)(milti_head_attention1)
    Maxpooling_layer1 = GlobalAveragePooling1D()(Conv_layer1)
    Maxpooling_layer2 = GlobalAveragePooling1D()(Conv_layer2)
    Maxpooling_layer3 = GlobalAveragePooling1D()(Conv_layer3)
    Concatenate_layer1 = Concatenate()([Maxpooling_layer1,Maxpooling_layer2])
    Concatenate_layer2 = Concatenate()([Concatenate_layer1,Maxpooling_layer3])
    Concatenate_layer2 = Concatenate()([Concatenate_layer98,Concatenate_layer2])
    Concatenate_layer2 = Dropout(0.2)(Concatenate_layer2)

    # milti_head_attention2 = mul_att(personality_embedding_layer,personality_embedding_layer)
    milti_head_attention2 = Self_Attention(300)(personality_embedding_layer)
    Bi_LSTM_Layer2 = Bidirectional(LSTM(200,return_sequences=True, dropout=0.2,
                                        recurrent_dropout=0.2,activation = 'tanh'))(milti_head_attention2)
    Conv_layer14 = Conv1D(128, 1, padding='valid', activation='relu', strides=1)(Bi_LSTM_Layer2)
    Conv_layer15 = Conv1D(128, 4, padding='valid', activation='relu', strides=1)(Bi_LSTM_Layer2)
    Conv_layer16 = Conv1D(128, 9, padding='valid', activation='relu', strides=1)(Bi_LSTM_Layer2)
    Maxpooling_layer14 = GlobalAveragePooling1D()(Conv_layer14)
    Maxpooling_layer15 = GlobalAveragePooling1D()(Conv_layer15)
    Maxpooling_layer16 = GlobalAveragePooling1D()(Conv_layer16)
    Concatenate_layer13 = Concatenate()([Maxpooling_layer14,Maxpooling_layer15])
    Concatenate_layer14 = Concatenate()([Concatenate_layer13,Maxpooling_layer16])


    Conv_layer4 = Conv1D(128, 1, padding='valid', activation='relu', strides=1)(milti_head_attention2)
    Conv_layer5 = Conv1D(128, 4, padding='valid', activation='relu', strides=1)(milti_head_attention2)
    Conv_layer6 = Conv1D(128, 9, padding='valid', activation='relu', strides=1)(milti_head_attention2)
    Maxpooling_layer4 = GlobalAveragePooling1D()(Conv_layer4)
    Maxpooling_layer5 = GlobalAveragePooling1D()(Conv_layer5)
    Maxpooling_layer6 = GlobalAveragePooling1D()(Conv_layer6)
    Concatenate_layer3 = Concatenate()([Maxpooling_layer4,Maxpooling_layer5])
    Concatenate_layer4 = Concatenate()([Concatenate_layer3,Maxpooling_layer6])
    Concatenate_layer4 = Concatenate()([Concatenate_layer4,Concatenate_layer14])
    Concatenate_layer4 = Dropout(0.2)(Concatenate_layer4)

    Concatenate_layer = Concatenate()([Concatenate_layer2,Concatenate_layer4])
    Flatten_layer = Flatten()(Concatenate_layer)
    # transformer_block = TransformerBlock(embed_dim=embedding_size,num_heads=2,ff_dim=32)
    # Transformer_layer1 = transformer_block(position_embedding_layer)
    # Global_AVG_Pooling_layer1 = GlobalAveragePooling1D()(Transformer_layer1)
    # Drop_layer1 = Dropout(0.5)(Global_AVG_Pooling_layer1)
    Drop_layer1 = Dropout(0.3)(Flatten_layer)
    # Dense_layer1 = Dense(64, activation='relu')(Drop_layer1)
    # Dense_layer2 = Dense(32, activation='relu')(Dense_layer1)
    Dense_layer1 = Dense(64, activation='tanh')(Drop_layer1)
    Dense_layer2 = Dense(32, activation='tanh')(Dense_layer1)
    # Dense_layer1 = Dense(64)(Drop_layer1)
    # Dense_layer1 = LeakyReLU()(Dense_layer1)
    # Dense_layer2 = Dense(32)(Dense_layer1)
    # Dense_layer2 = LeakyReLU()(Dense_layer2)
    output1 = Dense(1, activation='sigmoid')(Dense_layer2)
    output2 = Dense(1, activation='sigmoid')(Dense_layer2)
    output3 = Dense(1, activation='sigmoid')(Dense_layer2)
    output4 = Dense(1, activation='sigmoid')(Dense_layer2)
    output5 = Dense(1, activation='sigmoid')(Dense_layer2)
    return output1,output2,output3,output4,output5
